{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPl1QMYjkVogG/EaFmx9C5v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/usaidahmed01/Deep-Learning/blob/master/1_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y81F0cbfCyBI"
      },
      "outputs": [],
      "source": [
        "# Bag of Word => assign each word a unique index so like if two sentences have the same meaning but contain different words then bag of words dont know both sentences are same.\n",
        "\n",
        "# TF/IDF => check any one word then find how many times that word is used, tell about the word that it is common or rare\n",
        "\n",
        "# I love this movie\n",
        "# I adore this film\n",
        "  # Both have same meanings but for BOW these are different\n",
        "\n",
        "BOW => words into index\n",
        "TF/IDF => sort words by their frequency\n",
        "\n",
        "# WORD EMBEDDING\n",
        "  # Similar words qareeb qareeb hongay like ek cluster mein but difeerent words alag alag space mein hongay\n",
        "\n",
        "# Pros: model word ke meaning ko samjhega naake just assign the indexes\n",
        "Embedding(input_dim = 1000 , output_dim = 16 , input_length = 6)\n",
        "\n",
        "# input_dim = vocabulary size OR total unique words\n",
        "# output_dim = size of word vector (bigger: more expressive => detailed meaning, slower)\n",
        "\n",
        "word index -> lookup table -> vector(3d place mein kisi cluster mein embed)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RNN (Recurrent Neural Network)\n",
        "\n",
        "Input word -> Hidden state -> output\n",
        "           -> Memory\n",
        "\n",
        "# Cons: Forget about long term information\n",
        "\n",
        "LSTM => Long Short Term Memory\n",
        "  Solve the RNN memory problem also decides what should have learned and what should not be."
      ],
      "metadata": {
        "id": "jZMddehWP7-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RNN Example: I grew up in France and I speak fluent\n",
        "  - RNN forgets France by the time it reaches the blank\n",
        "  - Vanish and explode gradient problem due to back propagation\n",
        "\n",
        "Solution is LSTM: Long Short Term Memory\n",
        "  - It has a memory cell called Long term storage.\n",
        "  - Hidden State (Short-term working memory)\n",
        "  - It has 3 gates to control information flow.\n",
        "      - Forget gate (what to forget) values between 0 and 1 {0 means forget completely 1 means dont forget}\n",
        "      - Input gate (Decides what information is important, Decide how much to write)\n",
        "      - Output gate (Decides what part of  memory  becomes visible){\n",
        "          like 10 chaps yaad kiye then jou paper mein aya just wohi likha no fuzool talks}\n",
        "  - Long term dependencies are preserved.\n",
        "  - LSTM Forward Pass:\n",
        "      At every step:\n",
        "        1. Forget irrelevant memory\n",
        "        2. Add new important information\n",
        "        3. Output required information\n"
      ],
      "metadata": {
        "id": "zmqc6qdAz3vq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GRU: Gated Recurrent Unit (lite version of LSTM)\n",
        "  - it merges the Input and Forget gate\n",
        "  - Reduces complexity\n",
        "  - Train Faster\n",
        "  - Uses fewer parameters\n",
        "  - Gates:\n",
        "      - Update Gate: How much past information to keep and how much new information to add(Forget + Input)\n",
        "      - Reset Gate: How much past context to ignore"
      ],
      "metadata": {
        "id": "lI2HpqU83Ajc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "73n26PRKz10R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
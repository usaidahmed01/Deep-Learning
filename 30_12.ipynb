{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMCnzoi6yo87AwzgDy8F7nc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/usaidahmed01/Deep-Learning/blob/master/30_12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PEvbFmKz0Shu"
      },
      "outputs": [],
      "source": [
        "# Corpus\n",
        "# Each Sentence = Document\n",
        "texts = [\n",
        "    'I love this movie',\n",
        "    'This film is amazing',\n",
        "    'I enjoyed the movie',\n",
        "    'I hate this movie',\n",
        "    'This film was terrible',\n",
        "    'I disliked the movie'\n",
        "]\n",
        "labels = [1, 1, 1, 0, 0 , 0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(num_words=1000 , oov_token= '<OOV>') # oov_token means out of vocabulary\n",
        "\n",
        "tokenizer.fit_on_texts(texts) # assign indexes on each word\n"
      ],
      "metadata": {
        "id": "dhKs_GCy0tSn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_index = tokenizer.word_index\n",
        "print(word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzq8RKKj1N9j",
        "outputId": "fa1ed88c-a61e-46c4-d6e7-ade0476f544d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'<OOV>': 1, 'i': 2, 'this': 3, 'movie': 4, 'film': 5, 'the': 6, 'love': 7, 'is': 8, 'amazing': 9, 'enjoyed': 10, 'hate': 11, 'was': 12, 'terrible': 13, 'disliked': 14}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert Texts into sequences\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "print(sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCWLt8wE2DxL",
        "outputId": "86116758-3475-46ca-fd64-18fe61fb757f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2, 7, 3, 4], [3, 5, 8, 9], [2, 10, 6, 4], [2, 11, 3, 4], [3, 5, 12, 13], [2, 14, 6, 4]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Padding => for fixed input size\n",
        "# Neural network ko fix dimension deni hain thats why\n",
        "\n",
        "# we have done this Previously by vectorization function\n",
        "\n",
        "# Two Types of padding pre and post\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "max_length = 6\n",
        "X = pad_sequences(sequences , maxlen = max_length , padding = 'post')\n",
        "y = labels\n"
      ],
      "metadata": {
        "id": "PD3nNL8v26x4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from keras.layers import Embedding , Flatten, Dense\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(input_dim = 1000 , output_dim = 16 , input_length = max_length),\n",
        "    # Embedding is basically converts word id into dense vector like convert into metrix\n",
        "    Flatten(),\n",
        "    # Flatten just make the metrix into 1 dimension\n",
        "    Dense(16 , activation = 'relu'),\n",
        "    Dense(1 , activation = 'sigmoid')\n",
        "])"
      ],
      "metadata": {
        "id": "tVmA-tzY4Z82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77500ad4-f7ed-4116-80f9-7fd4cb53a425"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer = 'adam',\n",
        "    loss = 'binary_crossentropy',\n",
        "    metrics = ['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "1L7ZPHNQ52_J"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "y = np.array(y)\n",
        "history = model.fit(X , y , epochs = 30 , verbose = 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpb60Lax6gNT",
        "outputId": "58bfc77f-58e8-4fda-eebc-d760219462f9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "1/1 - 1s - 1s/step - accuracy: 0.8333 - loss: 0.6884\n",
            "Epoch 2/30\n",
            "1/1 - 0s - 208ms/step - accuracy: 0.8333 - loss: 0.6863\n",
            "Epoch 3/30\n",
            "1/1 - 0s - 38ms/step - accuracy: 0.8333 - loss: 0.6844\n",
            "Epoch 4/30\n",
            "1/1 - 0s - 65ms/step - accuracy: 0.8333 - loss: 0.6827\n",
            "Epoch 5/30\n",
            "1/1 - 0s - 38ms/step - accuracy: 0.8333 - loss: 0.6809\n",
            "Epoch 6/30\n",
            "1/1 - 0s - 41ms/step - accuracy: 0.8333 - loss: 0.6794\n",
            "Epoch 7/30\n",
            "1/1 - 0s - 62ms/step - accuracy: 0.8333 - loss: 0.6778\n",
            "Epoch 8/30\n",
            "1/1 - 0s - 62ms/step - accuracy: 0.8333 - loss: 0.6763\n",
            "Epoch 9/30\n",
            "1/1 - 0s - 51ms/step - accuracy: 0.8333 - loss: 0.6746\n",
            "Epoch 10/30\n",
            "1/1 - 0s - 48ms/step - accuracy: 0.8333 - loss: 0.6728\n",
            "Epoch 11/30\n",
            "1/1 - 0s - 48ms/step - accuracy: 1.0000 - loss: 0.6711\n",
            "Epoch 12/30\n",
            "1/1 - 0s - 61ms/step - accuracy: 1.0000 - loss: 0.6693\n",
            "Epoch 13/30\n",
            "1/1 - 0s - 56ms/step - accuracy: 1.0000 - loss: 0.6674\n",
            "Epoch 14/30\n",
            "1/1 - 0s - 52ms/step - accuracy: 1.0000 - loss: 0.6655\n",
            "Epoch 15/30\n",
            "1/1 - 0s - 67ms/step - accuracy: 1.0000 - loss: 0.6635\n",
            "Epoch 16/30\n",
            "1/1 - 0s - 60ms/step - accuracy: 1.0000 - loss: 0.6615\n",
            "Epoch 17/30\n",
            "1/1 - 0s - 62ms/step - accuracy: 1.0000 - loss: 0.6594\n",
            "Epoch 18/30\n",
            "1/1 - 0s - 53ms/step - accuracy: 1.0000 - loss: 0.6572\n",
            "Epoch 19/30\n",
            "1/1 - 0s - 33ms/step - accuracy: 1.0000 - loss: 0.6550\n",
            "Epoch 20/30\n",
            "1/1 - 0s - 63ms/step - accuracy: 1.0000 - loss: 0.6527\n",
            "Epoch 21/30\n",
            "1/1 - 0s - 37ms/step - accuracy: 1.0000 - loss: 0.6505\n",
            "Epoch 22/30\n",
            "1/1 - 0s - 39ms/step - accuracy: 1.0000 - loss: 0.6481\n",
            "Epoch 23/30\n",
            "1/1 - 0s - 38ms/step - accuracy: 1.0000 - loss: 0.6456\n",
            "Epoch 24/30\n",
            "1/1 - 0s - 38ms/step - accuracy: 1.0000 - loss: 0.6431\n",
            "Epoch 25/30\n",
            "1/1 - 0s - 38ms/step - accuracy: 1.0000 - loss: 0.6405\n",
            "Epoch 26/30\n",
            "1/1 - 0s - 39ms/step - accuracy: 1.0000 - loss: 0.6378\n",
            "Epoch 27/30\n",
            "1/1 - 0s - 36ms/step - accuracy: 1.0000 - loss: 0.6351\n",
            "Epoch 28/30\n",
            "1/1 - 0s - 35ms/step - accuracy: 1.0000 - loss: 0.6323\n",
            "Epoch 29/30\n",
            "1/1 - 0s - 41ms/step - accuracy: 1.0000 - loss: 0.6295\n",
            "Epoch 30/30\n",
            "1/1 - 0s - 39ms/step - accuracy: 1.0000 - loss: 0.6265\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_rev = ['I love this film',\n",
        "           'This movie wsas horrible']\n",
        "\n",
        "new_seq = tokenizer.texts_to_sequences(new_rev)\n",
        "new_padded = pad_sequences(new_seq , maxlen = max_length , padding = 'post')\n",
        "predictions = model.predict(new_padded)\n",
        "print(predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9y1PVSY67MJ",
        "outputId": "82a657e0-2bcc-450f-960a-29f162973e83"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
            "[[0.5277723]\n",
            " [0.5021459]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "model.save_weights('sentiment.weights.h5')\n",
        "print('Done')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOb7_3V69JEZ",
        "outputId": "1ff4b566-a4b1-4a53-8e95-873ecd6cb8d1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Re use the Same Model\n",
        "\n",
        "new_model = Sequential([\n",
        "    Embedding(input_dim = 1000 , output_dim = 16 , input_length = max_length),\n",
        "    Flatten(),\n",
        "    Dense(16 , activation = 'relu'),\n",
        "    Dense(1 , activation = 'sigmoid')\n",
        "])\n",
        "\n",
        "new_model.compile(\n",
        "    optimizer = 'adam',\n",
        "    loss = 'binary_crossentropy',\n",
        "    metrics = ['accuracy']\n",
        "    )\n",
        "\n",
        "new_model.build(input_shape = (None , max_length))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zP8VVurb9rvq",
        "outputId": "f6eb7f08-b436-47da-dd49-73e4a27993ce"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LDkmcp5cE-Oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_model.load_weights('sentiment.weights.h5')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIE52M1z_CbU",
        "outputId": "3498dac9-e09e-4056-9b28-7b4c37c3e8c7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/saving/saving_lib.py:802: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 12 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_model.save('sentiment_model.keras')"
      ],
      "metadata": {
        "id": "fxxB6ImVE_ll"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "new_model = keras.models.load_model('sentiment_model.keras')\n",
        "\n",
        "\n",
        "new_pred2 = new_model.predict(new_padded)\n",
        "print(new_pred2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTSFQ-iHFnS4",
        "outputId": "69ddf763-5b2c-4010-87d5-38998eda987c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
            "[[0.5277723]\n",
            " [0.5021459]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/saving/saving_lib.py:802: UserWarning: Skipping variable loading for optimizer 'adam', because it has 12 variables whereas the saved optimizer has 2 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "161f4f5c",
        "outputId": "a6bab2bd-cc58-4b54-ffe0-d4ce94913f2f"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from keras.layers import Embedding , Flatten, Dense\n",
        "import numpy as np\n",
        "\n",
        "texts_practice = [\n",
        "    'This album is a masterpiece, a true work of art from start to finish.',\n",
        "    'Every track on this record is pure gold; I can\\'t stop listening.',\n",
        "    'The vocals are breathtaking, and the instrumentation is incredibly rich.',\n",
        "    'A truly innovative sound that pushes the boundaries of the genre.',\n",
        "    'This artist has outdone themselves; a career-defining album.',\n",
        "    'I felt every emotion in these songs, truly captivating.',\n",
        "    'The production quality is superb, crystal clear and powerful.',\n",
        "    'An absolute must-listen for any music enthusiast.',\n",
        "    'This album brought me so much joy; it\\'s incredibly uplifting.',\n",
        "    'I\\'ve been waiting for a release like this for years; it\\'s perfect.',\n",
        "    'The beats are infectious, and the melodies are unforgettable.',\n",
        "    'A fantastic blend of old-school vibes and modern flair.',\n",
        "    'This record has quickly become my favorite of the year.',\n",
        "    'The lyrics are deeply moving and thought-provoking.',\n",
        "    'Such a vibrant and energetic album, perfect for any mood.',\n",
        "    'I discovered so many new layers with each listen.',\n",
        "    'The live performance of these tracks must be incredible.',\n",
        "    'Truly a timeless piece of music, will be remembered for decades.',\n",
        "    'I wholeheartedly recommend this to everyone; it\\'s brilliant.',\n",
        "    'A powerful and emotional journey from beginning to end.',\n",
        "    'This album is a complete disappointment, utterly forgettable.',\n",
        "    'I had high hopes, but this record fell completely flat.',\n",
        "    'The singing is off-key, and the instruments sound muddy.',\n",
        "    'A very generic and uninspired attempt at music.',\n",
        "    'This artist has clearly lost their touch; a terrible release.',\n",
        "    'I felt nothing while listening to these songs, utterly bland.',\n",
        "    'The production quality is poor, sounds cheap and rushed.',\n",
        "    'Avoid this album at all costs; a waste of time.',\n",
        "    'This album brought me sadness; it\\'s incredibly depressing.',\n",
        "    'I wish I hadn\\'t wasted my money on this; it\\'s awful.',\n",
        "    'The beats are monotonous, and the melodies are painful.',\n",
        "    'A confusing mix of styles that just doesn\\'t work.',\n",
        "    'This record might be the worst of the year.',\n",
        "    'The lyrics are shallow and repetitive.',\n",
        "    'Such a dull and lifeless album, nothing exciting.',\n",
        "    'I kept waiting for it to get good, but it never did.',\n",
        "    'I can\\'t imagine anyone enjoying this live.',\n",
        "    'Just another forgettable release in a sea of mediocrity.',\n",
        "    'I strongly advise against listening to this; it\\'s dreadful.',\n",
        "    'A chaotic and unpleasant experience from start to finish.',\n",
        "    'The sound engineering is horrendous, a painful listen.',\n",
        "    'This needs a full remix to be even remotely listenable.',\n",
        "    'Their previous work was so much better, what happened?',\n",
        "    'An embarrassment to the genre, truly awful.',\n",
        "    'I wouldn\\'t recommend this to my worst enemy.',\n",
        "    'The album drags on forever, lacking any standout moments.',\n",
        "    'Completely devoid of originality or creativity.',\n",
        "    'A truly jarring and unpleasant auditory experience.',\n",
        "    'This is background noise at best, nothing more.',\n",
        "    'I\\'m genuinely surprised this made it to release.'\n",
        "]\n",
        "\n",
        "labels_practice = [\n",
        "    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
        "    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "    0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
        "]\n",
        "\n",
        "print(f\"Generated {len(texts_practice)} practice texts and {len(labels_practice)} practice labels.\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated 50 practice texts and 50 practice labels.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6351f1f5"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   A new corpus of 50 mixed music reviews was created, consisting of 20 positive and 30 negative reviews, each with corresponding binary labels.\n",
        "*   The practice texts were successfully preprocessed by tokenizing and padding them to a `max_length` of 6, after ensuring the `tokenizer` was properly initialized.\n",
        "*   The `new_model` successfully generated sentiment predictions (probabilities) for the preprocessed practice corpus.\n",
        "*   The model achieved an accuracy of 0.5400 on the newly created practice corpus.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The accuracy of 0.5400 on the practice corpus suggests that the `new_model` performs only slightly better than random guessing for sentiment classification on this specific dataset, indicating poor generalization or an insufficient training corpus for music reviews.\n",
        "*   To improve model performance, consider retraining the model on a substantially larger and more diverse dataset of music reviews. Additionally, explore more advanced NLP techniques or neural network architectures better suited for sentiment analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizerP = Tokenizer(num_words = 2000 , oov_token='<OOV>')\n",
        "tokenizerP.fit_on_texts(texts_practice)\n",
        "\n",
        "\n",
        "word_indexes = tokenizerP.word_index\n",
        "word_indexes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZT4ctu4Uk9MW",
        "outputId": "c093a626-d3fb-42eb-c86f-4359f7e89123"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'<OOV>': 1,\n",
              " 'the': 2,\n",
              " 'this': 3,\n",
              " 'a': 4,\n",
              " 'and': 5,\n",
              " 'i': 6,\n",
              " 'of': 7,\n",
              " 'to': 8,\n",
              " 'album': 9,\n",
              " 'is': 10,\n",
              " 'are': 11,\n",
              " 'for': 12,\n",
              " \"it's\": 13,\n",
              " 'truly': 14,\n",
              " 'record': 15,\n",
              " 'release': 16,\n",
              " 'be': 17,\n",
              " 'work': 18,\n",
              " 'from': 19,\n",
              " 'on': 20,\n",
              " 'listening': 21,\n",
              " 'incredibly': 22,\n",
              " 'sound': 23,\n",
              " 'has': 24,\n",
              " 'these': 25,\n",
              " 'listen': 26,\n",
              " 'any': 27,\n",
              " 'music': 28,\n",
              " 'so': 29,\n",
              " 'my': 30,\n",
              " 'at': 31,\n",
              " 'nothing': 32,\n",
              " 'it': 33,\n",
              " 'start': 34,\n",
              " 'finish': 35,\n",
              " 'every': 36,\n",
              " \"can't\": 37,\n",
              " 'that': 38,\n",
              " 'genre': 39,\n",
              " 'artist': 40,\n",
              " 'felt': 41,\n",
              " 'in': 42,\n",
              " 'songs': 43,\n",
              " 'production': 44,\n",
              " 'quality': 45,\n",
              " 'powerful': 46,\n",
              " 'an': 47,\n",
              " 'must': 48,\n",
              " 'brought': 49,\n",
              " 'me': 50,\n",
              " 'much': 51,\n",
              " 'waiting': 52,\n",
              " 'perfect': 53,\n",
              " 'beats': 54,\n",
              " 'melodies': 55,\n",
              " 'year': 56,\n",
              " 'lyrics': 57,\n",
              " 'such': 58,\n",
              " 'live': 59,\n",
              " 'recommend': 60,\n",
              " 'utterly': 61,\n",
              " 'forgettable': 62,\n",
              " 'but': 63,\n",
              " 'completely': 64,\n",
              " 'their': 65,\n",
              " 'awful': 66,\n",
              " 'painful': 67,\n",
              " 'just': 68,\n",
              " 'worst': 69,\n",
              " 'unpleasant': 70,\n",
              " 'experience': 71,\n",
              " 'masterpiece': 72,\n",
              " 'true': 73,\n",
              " 'art': 74,\n",
              " 'track': 75,\n",
              " 'pure': 76,\n",
              " 'gold': 77,\n",
              " 'stop': 78,\n",
              " 'vocals': 79,\n",
              " 'breathtaking': 80,\n",
              " 'instrumentation': 81,\n",
              " 'rich': 82,\n",
              " 'innovative': 83,\n",
              " 'pushes': 84,\n",
              " 'boundaries': 85,\n",
              " 'outdone': 86,\n",
              " 'themselves': 87,\n",
              " 'career': 88,\n",
              " 'defining': 89,\n",
              " 'emotion': 90,\n",
              " 'captivating': 91,\n",
              " 'superb': 92,\n",
              " 'crystal': 93,\n",
              " 'clear': 94,\n",
              " 'absolute': 95,\n",
              " 'enthusiast': 96,\n",
              " 'joy': 97,\n",
              " 'uplifting': 98,\n",
              " \"i've\": 99,\n",
              " 'been': 100,\n",
              " 'like': 101,\n",
              " 'years': 102,\n",
              " 'infectious': 103,\n",
              " 'unforgettable': 104,\n",
              " 'fantastic': 105,\n",
              " 'blend': 106,\n",
              " 'old': 107,\n",
              " 'school': 108,\n",
              " 'vibes': 109,\n",
              " 'modern': 110,\n",
              " 'flair': 111,\n",
              " 'quickly': 112,\n",
              " 'become': 113,\n",
              " 'favorite': 114,\n",
              " 'deeply': 115,\n",
              " 'moving': 116,\n",
              " 'thought': 117,\n",
              " 'provoking': 118,\n",
              " 'vibrant': 119,\n",
              " 'energetic': 120,\n",
              " 'mood': 121,\n",
              " 'discovered': 122,\n",
              " 'many': 123,\n",
              " 'new': 124,\n",
              " 'layers': 125,\n",
              " 'with': 126,\n",
              " 'each': 127,\n",
              " 'performance': 128,\n",
              " 'tracks': 129,\n",
              " 'incredible': 130,\n",
              " 'timeless': 131,\n",
              " 'piece': 132,\n",
              " 'will': 133,\n",
              " 'remembered': 134,\n",
              " 'decades': 135,\n",
              " 'wholeheartedly': 136,\n",
              " 'everyone': 137,\n",
              " 'brilliant': 138,\n",
              " 'emotional': 139,\n",
              " 'journey': 140,\n",
              " 'beginning': 141,\n",
              " 'end': 142,\n",
              " 'complete': 143,\n",
              " 'disappointment': 144,\n",
              " 'had': 145,\n",
              " 'high': 146,\n",
              " 'hopes': 147,\n",
              " 'fell': 148,\n",
              " 'flat': 149,\n",
              " 'singing': 150,\n",
              " 'off': 151,\n",
              " 'key': 152,\n",
              " 'instruments': 153,\n",
              " 'muddy': 154,\n",
              " 'very': 155,\n",
              " 'generic': 156,\n",
              " 'uninspired': 157,\n",
              " 'attempt': 158,\n",
              " 'clearly': 159,\n",
              " 'lost': 160,\n",
              " 'touch': 161,\n",
              " 'terrible': 162,\n",
              " 'while': 163,\n",
              " 'bland': 164,\n",
              " 'poor': 165,\n",
              " 'sounds': 166,\n",
              " 'cheap': 167,\n",
              " 'rushed': 168,\n",
              " 'avoid': 169,\n",
              " 'all': 170,\n",
              " 'costs': 171,\n",
              " 'waste': 172,\n",
              " 'time': 173,\n",
              " 'sadness': 174,\n",
              " 'depressing': 175,\n",
              " 'wish': 176,\n",
              " \"hadn't\": 177,\n",
              " 'wasted': 178,\n",
              " 'money': 179,\n",
              " 'monotonous': 180,\n",
              " 'confusing': 181,\n",
              " 'mix': 182,\n",
              " 'styles': 183,\n",
              " \"doesn't\": 184,\n",
              " 'might': 185,\n",
              " 'shallow': 186,\n",
              " 'repetitive': 187,\n",
              " 'dull': 188,\n",
              " 'lifeless': 189,\n",
              " 'exciting': 190,\n",
              " 'kept': 191,\n",
              " 'get': 192,\n",
              " 'good': 193,\n",
              " 'never': 194,\n",
              " 'did': 195,\n",
              " 'imagine': 196,\n",
              " 'anyone': 197,\n",
              " 'enjoying': 198,\n",
              " 'another': 199,\n",
              " 'sea': 200,\n",
              " 'mediocrity': 201,\n",
              " 'strongly': 202,\n",
              " 'advise': 203,\n",
              " 'against': 204,\n",
              " 'dreadful': 205,\n",
              " 'chaotic': 206,\n",
              " 'engineering': 207,\n",
              " 'horrendous': 208,\n",
              " 'needs': 209,\n",
              " 'full': 210,\n",
              " 'remix': 211,\n",
              " 'even': 212,\n",
              " 'remotely': 213,\n",
              " 'listenable': 214,\n",
              " 'previous': 215,\n",
              " 'was': 216,\n",
              " 'better': 217,\n",
              " 'what': 218,\n",
              " 'happened': 219,\n",
              " 'embarrassment': 220,\n",
              " \"wouldn't\": 221,\n",
              " 'enemy': 222,\n",
              " 'drags': 223,\n",
              " 'forever': 224,\n",
              " 'lacking': 225,\n",
              " 'standout': 226,\n",
              " 'moments': 227,\n",
              " 'devoid': 228,\n",
              " 'originality': 229,\n",
              " 'or': 230,\n",
              " 'creativity': 231,\n",
              " 'jarring': 232,\n",
              " 'auditory': 233,\n",
              " 'background': 234,\n",
              " 'noise': 235,\n",
              " 'best': 236,\n",
              " 'more': 237,\n",
              " \"i'm\": 238,\n",
              " 'genuinely': 239,\n",
              " 'surprised': 240,\n",
              " 'made': 241}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequencesOnText = tokenizerP.texts_to_sequences(texts_practice)\n",
        "print(sequencesOnText)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4u0d7lAl_fv",
        "outputId": "761d7180-ac35-48a0-ec29-29f36be3ee8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[3, 9, 10, 4, 72, 4, 73, 18, 7, 74, 19, 34, 8, 35], [36, 75, 20, 3, 15, 10, 76, 77, 6, 37, 78, 21], [2, 79, 11, 80, 5, 2, 81, 10, 22, 82], [4, 14, 83, 23, 38, 84, 2, 85, 7, 2, 39], [3, 40, 24, 86, 87, 4, 88, 89, 9], [6, 41, 36, 90, 42, 25, 43, 14, 91], [2, 44, 45, 10, 92, 93, 94, 5, 46], [47, 95, 48, 26, 12, 27, 28, 96], [3, 9, 49, 50, 29, 51, 97, 13, 22, 98], [99, 100, 52, 12, 4, 16, 101, 3, 12, 102, 13, 53], [2, 54, 11, 103, 5, 2, 55, 11, 104], [4, 105, 106, 7, 107, 108, 109, 5, 110, 111], [3, 15, 24, 112, 113, 30, 114, 7, 2, 56], [2, 57, 11, 115, 116, 5, 117, 118], [58, 4, 119, 5, 120, 9, 53, 12, 27, 121], [6, 122, 29, 123, 124, 125, 126, 127, 26], [2, 59, 128, 7, 25, 129, 48, 17, 130], [14, 4, 131, 132, 7, 28, 133, 17, 134, 12, 135], [6, 136, 60, 3, 8, 137, 13, 138], [4, 46, 5, 139, 140, 19, 141, 8, 142], [3, 9, 10, 4, 143, 144, 61, 62], [6, 145, 146, 147, 63, 3, 15, 148, 64, 149], [2, 150, 10, 151, 152, 5, 2, 153, 23, 154], [4, 155, 156, 5, 157, 158, 31, 28], [3, 40, 24, 159, 160, 65, 161, 4, 162, 16], [6, 41, 32, 163, 21, 8, 25, 43, 61, 164], [2, 44, 45, 10, 165, 166, 167, 5, 168], [169, 3, 9, 31, 170, 171, 4, 172, 7, 173], [3, 9, 49, 50, 174, 13, 22, 175], [6, 176, 6, 177, 178, 30, 179, 20, 3, 13, 66], [2, 54, 11, 180, 5, 2, 55, 11, 67], [4, 181, 182, 7, 183, 38, 68, 184, 18], [3, 15, 185, 17, 2, 69, 7, 2, 56], [2, 57, 11, 186, 5, 187], [58, 4, 188, 5, 189, 9, 32, 190], [6, 191, 52, 12, 33, 8, 192, 193, 63, 33, 194, 195], [6, 37, 196, 197, 198, 3, 59], [68, 199, 62, 16, 42, 4, 200, 7, 201], [6, 202, 203, 204, 21, 8, 3, 13, 205], [4, 206, 5, 70, 71, 19, 34, 8, 35], [2, 23, 207, 10, 208, 4, 67, 26], [3, 209, 4, 210, 211, 8, 17, 212, 213, 214], [65, 215, 18, 216, 29, 51, 217, 218, 219], [47, 220, 8, 2, 39, 14, 66], [6, 221, 60, 3, 8, 30, 69, 222], [2, 9, 223, 20, 224, 225, 27, 226, 227], [64, 228, 7, 229, 230, 231], [4, 14, 232, 5, 70, 233, 71], [3, 10, 234, 235, 31, 236, 32, 237], [238, 239, 240, 3, 241, 33, 8, 16]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 50\n",
        "\n",
        "x = pad_sequences(sequencesOnText , max_length , padding = 'post')\n",
        "y = np.array(labels_practice)"
      ],
      "metadata": {
        "id": "lHcuTBkBmejG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelP = Sequential([\n",
        "    Embedding(input_dim = 2000 , output_dim= 16 , input_length = max_length),\n",
        "    Flatten(),\n",
        "    Dense(64 , activation = 'relu'),\n",
        "    Dense(1 , activation = 'sigmoid')\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfH-7duOm76n",
        "outputId": "d7c71d3d-7396-46d4-93ef-87c299d26e3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "modelP.compile(\n",
        "    optimizer = 'adam',\n",
        "    loss = 'binary_crossentropy',\n",
        "    metrics = ['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "jRyXWh2Bm9cG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelP.fit(x , y, epochs= 30 , verbose = 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjcpdWWSpJrv",
        "outputId": "05ecce8d-373e-42bc-e957-d9727c23ac8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "2/2 - 2s - 922ms/step - accuracy: 0.6000 - loss: 0.6808\n",
            "Epoch 2/30\n",
            "2/2 - 0s - 85ms/step - accuracy: 0.6000 - loss: 0.6671\n",
            "Epoch 3/30\n",
            "2/2 - 0s - 75ms/step - accuracy: 0.6000 - loss: 0.6625\n",
            "Epoch 4/30\n",
            "2/2 - 0s - 38ms/step - accuracy: 0.6000 - loss: 0.6638\n",
            "Epoch 5/30\n",
            "2/2 - 0s - 39ms/step - accuracy: 0.6000 - loss: 0.6577\n",
            "Epoch 6/30\n",
            "2/2 - 0s - 41ms/step - accuracy: 0.6000 - loss: 0.6512\n",
            "Epoch 7/30\n",
            "2/2 - 0s - 41ms/step - accuracy: 0.6000 - loss: 0.6465\n",
            "Epoch 8/30\n",
            "2/2 - 0s - 44ms/step - accuracy: 0.6000 - loss: 0.6377\n",
            "Epoch 9/30\n",
            "2/2 - 0s - 42ms/step - accuracy: 0.6000 - loss: 0.6312\n",
            "Epoch 10/30\n",
            "2/2 - 0s - 57ms/step - accuracy: 0.6000 - loss: 0.6241\n",
            "Epoch 11/30\n",
            "2/2 - 0s - 46ms/step - accuracy: 0.6000 - loss: 0.6143\n",
            "Epoch 12/30\n",
            "2/2 - 0s - 43ms/step - accuracy: 0.6000 - loss: 0.6065\n",
            "Epoch 13/30\n",
            "2/2 - 0s - 43ms/step - accuracy: 0.6400 - loss: 0.5915\n",
            "Epoch 14/30\n",
            "2/2 - 0s - 44ms/step - accuracy: 0.6400 - loss: 0.5779\n",
            "Epoch 15/30\n",
            "2/2 - 0s - 50ms/step - accuracy: 0.6400 - loss: 0.5631\n",
            "Epoch 16/30\n",
            "2/2 - 0s - 77ms/step - accuracy: 0.6400 - loss: 0.5471\n",
            "Epoch 17/30\n",
            "2/2 - 0s - 45ms/step - accuracy: 0.6800 - loss: 0.5299\n",
            "Epoch 18/30\n",
            "2/2 - 0s - 46ms/step - accuracy: 0.7600 - loss: 0.5093\n",
            "Epoch 19/30\n",
            "2/2 - 0s - 29ms/step - accuracy: 0.8200 - loss: 0.4877\n",
            "Epoch 20/30\n",
            "2/2 - 0s - 49ms/step - accuracy: 0.8600 - loss: 0.4658\n",
            "Epoch 21/30\n",
            "2/2 - 0s - 33ms/step - accuracy: 0.8800 - loss: 0.4407\n",
            "Epoch 22/30\n",
            "2/2 - 0s - 28ms/step - accuracy: 0.9000 - loss: 0.4153\n",
            "Epoch 23/30\n",
            "2/2 - 0s - 33ms/step - accuracy: 0.9800 - loss: 0.3896\n",
            "Epoch 24/30\n",
            "2/2 - 0s - 31ms/step - accuracy: 0.9800 - loss: 0.3633\n",
            "Epoch 25/30\n",
            "2/2 - 0s - 39ms/step - accuracy: 1.0000 - loss: 0.3376\n",
            "Epoch 26/30\n",
            "2/2 - 0s - 31ms/step - accuracy: 1.0000 - loss: 0.3119\n",
            "Epoch 27/30\n",
            "2/2 - 0s - 30ms/step - accuracy: 1.0000 - loss: 0.2865\n",
            "Epoch 28/30\n",
            "2/2 - 0s - 33ms/step - accuracy: 1.0000 - loss: 0.2607\n",
            "Epoch 29/30\n",
            "2/2 - 0s - 32ms/step - accuracy: 1.0000 - loss: 0.2366\n",
            "Epoch 30/30\n",
            "2/2 - 0s - 32ms/step - accuracy: 1.0000 - loss: 0.2132\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7b731b912090>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_revB = ['This album is a complete disappointment, utterly forgettable',\n",
        "            'This album is so good, like its a masterpiece']\n",
        "\n",
        "new_rev_seq = tokenizerP.texts_to_sequences(new_revB)\n",
        "new_rev_padd = pad_sequences(new_rev_seq , max_length , padding = 'post')\n",
        "\n",
        "predictions = modelP.predict(new_rev_padd)\n",
        "print(predictions)\n",
        "print(['Good' if p >= 0.5 else 'bad' for p in predictions])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PyblHYnMpVzW",
        "outputId": "eb92c6a1-4803-4e98-9232-53d8c5efd166"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
            "[[0.12009695]\n",
            " [0.25812778]]\n",
            "['bad', 'bad']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N5v-hM8mq7hI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
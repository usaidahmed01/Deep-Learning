{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOxborNioTlvnQvNHs1lgML",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/usaidahmed01/Deep-Learning/blob/master/DL27_11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "M1Py1dAyyxtT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "eb6a6ce4-1105-4600-df4c-7e896c92b503"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-3909661273.py, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-3909661273.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    NLP: Natural Language Processing\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "NLP: Natural Language Processing\n",
        "\n",
        "1. Corpus\n",
        "  A large collection of text data.\n",
        "  for example: whatsapp chat (only text). large data warehouse\n",
        "\n",
        "2. Document\n",
        "  A single text data from corpus.\n",
        "  for example: whatsapp specific person chat.\n",
        "\n",
        "3. Vocabulary\n",
        "  A Document break down into words\n",
        "  for example: each word. (unique words)\n",
        "  'cat sat on mat'\n",
        "  ['cat' , 'sat' , 'on' , 'mat']\n",
        "\n",
        "Tokenization:(after all the below steps it will called Bag of Words => Numeric Vectors)\n",
        "  . Lemmatization\n",
        "      for example: Simplify words grammatically like \"Runs\" => \"Run\" , \"Better\" => \"Good\"\n",
        "  . Stemming\n",
        "    for example: if like the word is \"running\" it will be \"run\", \"Studying\" into \"Studi\"\n",
        "  . Stop Words Removal: Words if we remove them it wont affect our sentences.\n",
        "  . Remove the Repititions of words.\n",
        "  . Indexes will be assigned to each word.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TF - IDF(Term Frequency - Inverse Document Frequency)\n",
        "\n",
        "#   For example: 'Cat is not Happy' if we remove the stop words in this so it will be like \"cat is happy\"(sentence meaning changed) so use TF - IDF\n",
        "\n",
        "#   Words that are frequently used their importance and prioritization will be decreased and if words are rarely appeared then their prioritization will be increased (jou bht rarely aaraha hai uski importance barhjayegi)\n",
        "\n",
        "Tf = (Number of times term t appears in a document) / (Total number of terms in the document)\n",
        "IDF = log(Total number of documents / Number of documents with term t in it)\n",
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "corpus = [\n",
        "    'cat sat on the mat',\n",
        "    'dog chased the cat',\n",
        "    'cat is happy',\n",
        "]\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "print(vectorizer.get_feature_names_out())\n",
        "print('\\n' , X.toarray())\n",
        "# Will get the sparse metrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oinvvvCT66Cn",
        "outputId": "563685da-a230-4508-9955-af536649c967"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['cat' 'chased' 'dog' 'happy' 'is' 'mat' 'on' 'sat' 'the']\n",
            "\n",
            " [[0.29803159 0.         0.         0.         0.         0.50461134\n",
            "  0.50461134 0.50461134 0.38376993]\n",
            " [0.34520502 0.5844829  0.5844829  0.         0.         0.\n",
            "  0.         0.         0.44451431]\n",
            " [0.38537163 0.         0.         0.65249088 0.65249088 0.\n",
            "  0.         0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EZwcnlHu8OsO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
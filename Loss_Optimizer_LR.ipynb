{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMCRwtvKEZF30z/R0Ct3RGJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/usaidahmed01/Deep-Learning/blob/master/Loss_Optimizer_LR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjw9QFZlokMu"
      },
      "outputs": [],
      "source": [
        "# Loss Function\n",
        "\n",
        "How wrong is the model?\n",
        "\n",
        "- Model predicts -> loss measures mistake\n",
        "- Training tries to minimize the loss\n",
        "- No loss -> no learning\n",
        "\n",
        "For Example: youre shooting arrows at a target:\n",
        "  - loss: distance from bullseye\n",
        "  - optimizer: how you walk towards the bullseye\n",
        "\n",
        "# go through the Loss Function image (Problem , Output, Loss Function)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MSE: (loss = 'mse') # Square the error.\n",
        "Used in:\n",
        " - House Prices\n",
        " - Temperature measuring\n",
        " - Highly sensitive to outliers\n",
        "\n",
        "MAE: (loss = 'mae') # Calculate the absolute data.\n",
        "Used in:\n",
        " - if no clean data\n",
        " - Data has outliers then we will use it\n",
        " - Used for noisy real-world data or when outliers exist\n",
        "\n",
        "Binary Cross Entropy: (loss = 'binary_crossentropy') # Sentiment Analysis\n",
        "Used in:\n",
        "  - Fraud Detection\n",
        "  - Spam Detection\n",
        "  - Medical Diagnosis (Disease hai ya nahi)\n",
        "\n",
        "# When we talk about the classification hamesha base jou hoti hai wo Probability hoti hai distance waghera nahi hoti\n",
        "\n",
        "Crossentropy:\n",
        "- punishes confident wrong predictions Hard\n",
        "- encourages correct probabilty estimation\n",
        "\n",
        "Categorical crossentropy: (loss = 'categorical_crossentropy')\n",
        "Used in:\n",
        "  - labels are one-hot encoded\n",
        "  - Multi-class classification\n",
        "\n",
        "Sparse categorical crossentropy: (loss = 'sparse_categorical_crossentropy')\n",
        "  - same as categorical crossentropy but it is (memory efficient)"
      ],
      "metadata": {
        "id": "V0RWyGP_rOJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizers\n",
        "\n",
        "Optimizer decides:\n",
        "  - how big a step to take\n",
        "  - in which direction\n",
        "  - how fast to learn (learning rate)\n",
        "  - how stable training is\n",
        "\n",
        "loss = Destination\n",
        "Optimizer = navigation system"
      ],
      "metadata": {
        "id": "qlnt7ALerSke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Types of Optimizers:\n",
        "\n",
        "SGD Stochastic Gradient Descent: (optimizer = 'sgd')\n",
        "Characteristics: Simple , Slow , no memory\n",
        "Problem: Gets stuck , zig-zags\n",
        "\n",
        "SGD with momentum\n",
        "  - uses past gradients (has memory)\n",
        "  - reduces zig-zag\n",
        "  - moves faster in right direction\n",
        "\n",
        "RMSProp\n",
        "  - adjust learning rate per parameter\n",
        "  - prevents exploding gradients\n",
        "\n",
        "Set for:\n",
        "  - RNNs\n",
        "  - NLP\n",
        "\n",
        "Adam: RMSProp + Momentum (optimizer = Adam(learning_rate = 0.001))\n",
        "  - Fast\n",
        "  - Stable\n",
        "  - Handles sparse gradients\n",
        "  Problem: vanish the gradients\n",
        "\n",
        "Adam w: prevents vanishing gradients\n",
        "\n",
        "Learning Rate:\n",
        "  - if too high loss explodes\n",
        "  - if too low no learning"
      ],
      "metadata": {
        "id": "1tC1LePaxaOt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}